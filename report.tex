\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{natbib}
\pagestyle{fancy}
\lhead{SID: 309201470}
\rhead{SID: 310182212}

\title{COMP3608 Artificial Intelligence (Adv): Othello}
\author{Elizaveta Lisa Fedorenko: 309201470
\\ Tianyu Pu: 310182212}
\date{\today}
\fancyhead[L]{309201470}
\fancyhead[R]{310182212}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction and Background}
\subsection{Othello: The Game}
Othello (also commonly known as Reversi), is a two-player strategy board game played on an 8x8 board.
The game begins with four pieces in the centre, two for each player's colour. Both players take turns
to place a piece in their colour onto the board, flipping over all the opponent's pieces that are caught
or `flanked' between the new piece and another of the player's colour. The move is only valid if at
least one token was flanked in any direction. The game ends when the board is filled and/or when there are
no more possible moves for either player.

\subsection{Interpretation of the Rules}
There are a number of variations on the rules of Othello that are used in different situations such as
tournaments. This section outlines the version of the rules that was used to create the game.
\begin{enumerate}
 \item You can only place a piece on a tile that is empty and is in one of the eight squares immediately
 surrounding a piece of the opponent's colour.
 \item You can flip the opponent's pieces that are directly flanked by your new piece (the piece just
 put down) and the nearest piece of your colour in any of the eight directions.
 \item A move is only valid if you flip/ or flank at least one of your opponents pieces.
 \item A player may not pass unless they have no more valid moves: in this case they automatically pass.
\end{enumerate}

\subsection{Playing the game}
Our version of the game begins by asking the player whether they wish to verse a human or else which of the three AIs they wish to play. Input is tested to be one of the letters A, B, C or H. If invalid input is given the prompt is given again. They are then asked for their name (or both their names). As the name can be any value this is not tested (It is only included to make the input more personal).
The player is then told which colour they are (the game is symetric so this doesn't limit the game choices). The game defaults to human playing first. The rest of the user input requires the user to type in their move of the form 'x y' eg. 1 2. This is tested to be a valid move. It is not tested to be correct syntax. This is assumed (as per assignment spec).
In order to make the game more user friendly, at each stage your valid moves are printed out as a list, so you can chose a move. And all the moves played so far are also printed. When the AI moves you also see their valid moves and which move they choose. This makes games more recordable and user-friendly.

\section{Implementation of Strategies}
\subsection{Heuristic}
The game of Othello is complex ~\cite{post}. We therefore designed two evaluation functions. Strategy A and B use a very simple intuitive evaluation strategy. The heuristic simply counts the number of their tokens on the board and aims to maximise it. However for strategy C we implemented a far more complicated heuristic function that takes much more game strategy into consideration. There are three main methods to judge a position in Othello as follows ~\cite{strategy}:
\begin{enumerate}
\item \emph{Number of tokens} Obviously as the aim of the game is to have the most tokens, this is a good basic estimate of who is winning at any one point in time.
\item \emph{Stability} Certain pieces are a lot easier to `flank' than others. In particular corners are very stable and cannot be flipped. Pieces protruding from corners are also more stable.
\item \emph{Mobility} Having more moves available means you are better positioned strategically and less constricted.
\end{enumerate}
Each of these methods has its own shortcomings and advantages. We chose to implement two heuristics. The first is a basic one (eval1()) which simply uses the number of tokens to value its position. The second much `smarter' AI uses a far more complex heuristic (eval2()), which can be represented thus:
\begin{equation}
h(\mbox{black}) = \mbox{number black pieces}*f(\mbox{pieces}) + \mbox{factor}*\mbox{number available moves}
\end{equation}
Where f() is a function of the stability of each piece. f() weights corner pieces and pieces joint to corner pieces higher than centre unprotected pieces. The factor is a constant which chooses the balance to be given between each of the strategy calculations.
TALK ABOUT HOW BOTH HEURISTICS ARE ADMISSABLE

\subsection{Strategy A}
Strategy A uses the minimax algorithm. This has been implemented recursively. We have allowed the depth explored by the algorithm to vary depending on the level (1, 2 or 3) chosen by the user. The algorithm can be summarised thus:
\begin{enumerate}
\item Perform a depth-first search either to the terminal state or to the value $depth = level*2+1$, whichever comes first.
\item Evaluate the value of the heuristic at each terminal state.
\item Return the minimax value. Return MIN if it is the opponents move, and MAX heuristic at that level if it is your move.
\item Select the move with the maximum of the minimax values of the first level of children.
\end{enumerate}
Note this assumes that min will play optimally in each turn.

\subsection{Strategy B}
Strategy B is very similar to strategy A. The exception is it avoids exploring some paths, or prunes some paths. This allows it to explore deeper. The basic algorithm can be summarised thus:

\begin{enumerate}
\item Traverse the tree in depth first search order.
\item At each non-leaf node store the 'best so far' value: max is the $\alpha$ or the best maximum value so far, min is the $\beta$ or the best minimum value found so far.
\item Don't explore below a node n if either: n is a MAX node and $\alpha(n) \geq beta(i)$ for some MIN node i that is an ancestor of n. This is the $\beta$ cutoff. OR: n is a MIN node and $\beta(n) \leq \alpha(i)$ for some MAX node i that is an ancestor of n. This is the $\alpha$ cutoff.
\end{enumerate}
Note this assumes that min will play optimally in each turn.

\subsection{Strategy C}
For strategy C we decided to implement a non-deterministic strategy. We ran the mini-max algorithm and selected the best three outcomes (ranked). We then returned one of the moves based on a random card draw where each solution was weighted based on its rank thus:
\begin{enumerate}
\item The best move had an 80\% chance of being selected.
\item The second best move had a 15\% chance of being selected.
\item The third best move had a 5\% chance of being selected.
\end{enumerate}
Although this AI does not always select the optimal move, it is non-deterministic and thus harder to predict. We felt that the other two strategies would be beatable by experienced players who could predict where the AI would move next and thus adjust their strategy. The element of chance added here doesn't allow such calculations. Further, sometimes taking a risky move may lead to a more optimal final state.

\section{Empirical Setup}

\section{Findings}
\subsection{expectation}
We expected strategy A to perform the worst as it didn't search as deeply and didn't have a great heuristic. We also expected strategy B to have shortcomings due to the predictability as well as the more basic heuristic function used. Strategy C we expected would have very few losses. This is because of a number of factors. Firstly it is random, and thus cannot be predicted by a clever opponent. Secondly and most importantly it uses a much more accurate heuristic function.
\subsection{Results}
We ran THIS MANY GAMES and found that strategy BLAH performed best winning X percent as opposed to the Y percent or Z percent

\subsection{Conclusion}

\section{Reflection}

\subsection{Lisa's Reflection: 309201470}

I really enjoyed this assignment and felt that I learnt a lot from it. It was very satisfying to implement a working game and strategies. I particularly liked the open-ended-ness which meant that we could get creative with the third strategy. The most valuable thing I felt I learnt from this assignment is the complexity of AI. This complexity arrises from user-input and makes it very difficult to create an ideal opponent. The human opponent will act very differently based on his/ her level of expertice and will often not chose the optimal strategy. Thus it makes the task of the AI much more complex in moving optimally. Further it made me consider the importance of chance. Choosing the most strategically sound option each time is very predictable and is not an optimal strategy. In addition I learnt a lot about the game of Othello and the many complicated strategies involved.

\subsection{Tian's Reflection: 310182212}

\section{Extras}

\subsection{Multi-player}
In addition to the assignment requirements we implemented a human-vs-human game option.

\subsection{Possible moves}
We print out a list of possible moves for each player to make the game easier and more pleasant to play.

\section{Appendix}

\subsection{Strategy A}

\subsection{Strategy B}

\subsection{Strategy C}

\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
